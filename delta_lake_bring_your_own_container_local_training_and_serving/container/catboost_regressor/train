#!/usr/bin/env python3

# A sample training component that trains a simple CatBoost Regressor tree model.
# This implementation works in File mode and makes no assumptions about the input file names.

import os
import json
import traceback
import sys

from catboost import CatBoostRegressor
import numpy as np
import pandas as pd
import pyspark
from delta import *

prefix = '/opt/ml/'
input_path = prefix + 'input/data'
train_channel_name = 'train'
validation_channel_name = 'validation'

output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
model_file_name = 'catboost-regressor-model.dump'
train_path = os.path.join(input_path, train_channel_name)
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')


# The function to execute the training.
def train():
    print('Starting the training.')

    builder = pyspark.sql.SparkSession.builder.appName("MyApp") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

    spark = configure_spark_with_delta_pip(builder).getOrCreate()

    print(f'Using Spark Version: {spark.version}')

    try:
        # Read in any hyperparameters that the user passed with the training job
        print('Reading hyperparameters data: {}'.format(param_path))
        with open(param_path) as json_file:
            hyperparameters_data = json.load(json_file)
        print(f'hyperparameters_data: {hyperparameters_data}')

        # Take the set of train files and read them all into a single pandas dataframe
        california_housing_from_delta_df = spark.read.format("delta").load(train_path)
        print('Fetched training dataset from Delta Lake')
        california_housing_from_delta_df.show()

        train_df = california_housing_from_delta_df.toPandas()

        # Assumption is that the label is the last column
        print('building training dataset')
        X_train = train_df.iloc[:, :-1].values
        y_train = train_df.iloc[:, -1:].values

        # define and train model
        model = CatBoostRegressor()

        model.fit(X_train, y_train)

        # persist model
        path = os.path.join(model_path, model_file_name)
        print('saving model file to {}'.format(path))
        model.save_model(path)

        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc)
        # A non-zero exit dependencies causes the training job to be marked as Failed.
        sys.exit(255)


if __name__ == '__main__':
    train()

    # A zero exit dependencies causes the job to be marked a Succeeded.
    sys.exit(0)
