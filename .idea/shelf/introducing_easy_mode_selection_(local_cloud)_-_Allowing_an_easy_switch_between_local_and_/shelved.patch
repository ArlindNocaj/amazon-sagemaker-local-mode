Index: tensorflow_script_mode_local_training_and_serving/tensorflow_script_mode_local_training_and_serving.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This is a sample Python program that trains a simple TensorFlow CIFAR-10 model.\n# This implementation will work on your *local computer* or in the *AWS Cloud*.\n# To run training and inference *locally* set: `config = get_config(LOCAL_MODE)`\n# To run training and inference on the *cloud* set: `config = get_config(CLOUD_MODE)` and set a valid IAM role value in get_config()\n#\n# Prerequisites:\n#   1. Install required Python packages:\n#      `pip install -r requirements.txt`\n#   2. Docker Desktop installed and running on your computer:\n#      `docker ps`\n#   3. You should have AWS credentials configured on your local machine\n#      in order to be able to pull the docker image from ECR.\n##############################################################################################\n\nimport os\n\nimport boto3\nimport numpy as np\nimport sagemaker.session\nfrom sagemaker.local import LocalSession\nfrom sagemaker.tensorflow import TensorFlow\n\nLOCAL_MODE = 'LOCAL_MODE'\nCLOUD_MODE = 'CLOUD_MODE'\n\nDUMMY_IAM_ROLE = 'arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001'\ndata_files_list = ('train_data.npy', 'train_labels.npy', 'eval_data.npy', 'eval_labels.npy')\n\n\ndef download_training_and_eval_data():\n    if os.path.isfile('./data/train_data.npy') and \\\n            os.path.isfile('./data/train_labels.npy') and \\\n            os.path.isfile('./data/eval_data.npy') and \\\n            os.path.isfile('./data/eval_labels.npy'):\n        print('Training and evaluation datasets exist. Skipping Download')\n    else:\n        print('Downloading training and evaluation dataset')\n        s3 = boto3.resource('s3')\n        for filename in data_files_list:\n            s3.meta.client.download_file('sagemaker-sample-data-us-east-1', 'tensorflow/mnist/' + filename,\n                                         './data/' + filename)\n\n\ndef do_inference_on_local_endpoint(predictor, mode):\n    print(f'\\nStarting Inference on endpoint ({mode}).')\n    correct_predictions = 0\n\n    train_data = np.load('./data/train_data.npy')\n    train_labels = np.load('./data/train_labels.npy')\n\n    predictions = predictor.predict(train_data[:50])\n    for i in range(0, 50):\n        prediction = np.argmax(predictions['predictions'][i])\n        label = train_labels[i]\n        print('prediction is {}, label is {}, matched: {}'.format(prediction, label, prediction == label))\n        if prediction == label:\n            correct_predictions = correct_predictions + 1\n\n    print('Calculated Accuracy from predictions: {}'.format(correct_predictions / 50))\n\n\ndef upload_data_to_s3(bucket, prefix):\n    # Required if running in cloud mode. Skips upload if file exist in S3\n    s3 = boto3.resource('s3')\n    result = s3.meta.client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n    existing_files = [item['Key'] for item in result['Contents']] if 'Contents' in result else []\n    for filename in data_files_list:\n        if prefix + filename not in existing_files:\n            print('Uploading ' + filename + ' to s3://' + bucket + '/' + prefix + filename)\n            s3.meta.client.upload_file('./data/' + filename, bucket, prefix + filename)\n        else:\n            print('File already in bucket. Skipping uploading for: ' + prefix + filename)\n\n\ndef get_config(mode):\n    assert mode is CLOUD_MODE or mode is LOCAL_MODE, f'unknown mode selected: {mode}'\n\n    if mode == CLOUD_MODE:\n        ## REPLACE WITH A VALID IAM ROLE - START ##\n        role = DUMMY_IAM_ROLE\n        ## REPLACE WITH A VALID IAM ROLE - END ##\n        assert role is not DUMMY_IAM_ROLE, \"For cloud mode set a valid sagemaker iam role\"\n\n        print('Will run training on an ML instance in AWS.')\n        session = sagemaker.Session()\n        bucket = session.default_bucket()\n        s3_data_prefix = 'tensorflow_script_mode_cloud_training/mnist/'\n        instance_type = 'ml.m5.large'\n        training_dataset_path = 's3://' + bucket + '/' + s3_data_prefix\n\n    else:  # mode == LOCAL_MODE\n        print('Will run training locally in a container image.')\n        session = LocalSession()\n        session.config = {'local': {'local_code': True}}\n        instance_type = 'local'\n        training_dataset_path = \"file://./data/\"\n        role = DUMMY_IAM_ROLE  # not needed in local training\n        s3_data_prefix = None  # not needed in local training\n        bucket = None  # not needed in local training\n\n    config = {\n        'mode': mode,\n        's3_data_prefix': s3_data_prefix,\n        'sagemaker_session': session,\n        'bucket': bucket,\n        'instance_type': instance_type,\n        'training_dataset_path': training_dataset_path,\n        'role': role}\n    return config\n\n\ndef main():\n    config = get_config(LOCAL_MODE)\n    #config = get_config(CLOUD_MODE)\n\n    download_training_and_eval_data()\n\n    if config['mode'] is CLOUD_MODE:\n        upload_data_to_s3(config['bucket'], config['s3_data_prefix'])\n\n    print('Starting model training.')\n    print(\n        'Note: if launching for the first time in local mode, container image download might take a few minutes to complete.')\n    mnist_estimator = TensorFlow(entry_point='mnist_tf2.py',\n                                 role=config['role'],\n                                 instance_count=1,\n                                 instance_type=config['instance_type'],\n                                 framework_version='2.3.0',\n                                 py_version='py37',\n                                 distribution={'parameter_server': {'enabled': True}})\n\n    mnist_estimator.fit(config['training_dataset_path'])\n    print('Completed model training')\n\n    print('Deploying endpoint in ' + config['mode'])\n    predictor = mnist_estimator.deploy(initial_instance_count=1, instance_type=config['instance_type'])\n\n    do_inference_on_local_endpoint(predictor, config['mode'])\n\n    print('About to delete the endpoint to stop paying (if in cloud mode).')\n    predictor.delete_endpoint(predictor.endpoint_name)\n    predictor.delete_model()\n\n\nif __name__ == \"__main__\":\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- tensorflow_script_mode_local_training_and_serving/tensorflow_script_mode_local_training_and_serving.py	(revision 3df284072f19c44d961b4fa2010a5de98c84eb40)
+++ tensorflow_script_mode_local_training_and_serving/tensorflow_script_mode_local_training_and_serving.py	(date 1606144022953)
@@ -10,7 +10,7 @@
 #      `docker ps`
 #   3. You should have AWS credentials configured on your local machine
 #      in order to be able to pull the docker image from ECR.
-##############################################################################################
+###############################################################################################
 
 import os
 
